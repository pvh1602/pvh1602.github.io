---
---

@string{aps = {American Physical Society,}}


@article{pham2025graphon,
  abbr = {NeurIPS},
  title={The Graphon Limit Hypothesis: Understanding Neural Network Pruning via Infinite Width Analysis},
  author={Pham, Hoang and Ta, The-Anh and Jacobs, Tom and Burkholz, Rebekka and Tran-Thanh, Long},
  journal={Advances in Neural Information Processing Systems},
  year={2025},
  honor = {Spotlight Presentation},
  selected={true},
  abstract = {Sparse neural networks promise efficiency, yet training them effectively remains a fundamental challenge. Despite advances in pruning methods that create sparse architectures, understanding why some sparse structures are better trainable than others with the same level of sparsity remains poorly understood. Aiming to develop a systematic approach to this fundamental problem, we propose a novel theoretical framework based on the theory of graph limits, particularly graphons, that characterizes sparse neural networks in the infinite-width regime. Our key insight is that connectivity patterns of sparse neural networks induced by pruning methods converge to specific graphons as networks' width tends to infinity, which encodes implicit structural biases of different pruning methods. We postulate the Graphon Limit Hypothesis and provide empirical evidence to support it. Leveraging this graphon representation, we derive a Graphon Neural Tangent Kernel (Graphon NTK) to study the training dynamics of sparse networks in the infinite width limit. Graphon NTK provides a general framework for the theoretical analysis of sparse networks. We empirically show that the spectral analysis of Graphon NTK correlates with observed training dynamics of sparse networks, explaining the varying convergence behaviours of different pruning methods. Our framework provides theoretical insights into the impact of connectivity patterns on the trainability of various sparse network architectures.},
}

@article{xiang2025dpai,
  abbr = {ICLR},
  title={{DP}aI: Differentiable Pruning at Initialization with Node-Path Balance Principle},
  author={Xiang, Lichuan and Nguyen-Tri, Quan and Nguyen, Lan-Cuong and Pham, Hoang and Than, Khoat and Tran-Thanh, Long and Wen, Hongkai},
  journal={The Thirteenth International Conference on Learning Representations},
  year={2025},
  url={https://openreview.net/forum?id=hvLBTpiDt3},
  selected={false}
}

@article{Pham_2024_ECCV,
  abbr = {ECCV},
  author = {Pham, Hoang and Ta, The-Anh and Tran, Anh and Doan, Khoa D},
  title = {Flatness-aware Sequential Learning Generates Resilient Backdoors},
  journal = {Proceedings of the European Conference on Computer Vision (ECCV)},
  year = {2024},
  honor = {Oral Presentation},
  selected={true}
}


@article{pham2023towards,
  abbr = {NeurIPS},
  title={Towards Data-Agnostic Pruning At Initialization: What Makes a Good Sparse Mask?},
  author={Pham, Hoang and Ta, The-Anh and Liu, Shiwei and Xiang, Lichuan and Le, Dung D. and Wen, Hongkai and Tran-Thanh, Long},
  journal={Advances in Neural Information Processing Systems},
  year={2023},
  selected={true}
}

@article{nguyen2022adaptive,
  abbr = {Journal},
  title={Adaptive infinite dropout for noisy and sparse data streams},
  author={Nguyen*, Ha and Pham*, Hoang and Nguyen, Son and Van Linh, Ngo and Than, Khoat},
  journal={Machine Learning},
  pages={1--36},
  year={2022},
  publisher={Springer},
  selected = {false}
}

@article{phampruning,
  abbr = {ICML Workshop},
  title={Pruning Deep Equilibrium Models},
  author={Pham*, Hoang and Nguyen*, Tuc and Ta-The, Anh and Le, Dung D. and Tran-Thanh, Long},
  journal={ICML 2022 Workshop on Sparse Neural Networks},
  year={2022},
  selected = {false}
}

@article{van2022auxiliary,
  abbr = {PAKDD},
  title={Auxiliary local variables for improving regularization/prior approach in continual learning},
  author={Van*, Linh Ngo and Hai*, Nam Le and Pham*, Hoang and Than, Khoat},
  journal={Pacific-Asia Conference on Knowledge Discovery and Data Mining},
  pages={16--28},
  year={2022},
  organization={Springer},
  selected = {false}
}

@article{phamhypersparse,
  abbr = {Preprint},
  title={HyperSparse: Specializing Parameters of Meta-Learning Models for Effective User Cold-Start Recommendation},
  author={Pham, Hoang and Pham, Quang and Le, Dung D and Ta-The, Anh},
  journal={preprint},
  year = {2023},
  selected = {false}
}

